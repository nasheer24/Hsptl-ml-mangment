{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "116168bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64e826ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\parik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?', '!']\n",
    "data_file = open('health.json').read()\n",
    "intents = json.loads(data_file)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for intent in intents['diseases']:\n",
    "    for pattern in intent['symptoms']:\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        words.extend(w)\n",
    "        documents.append((w, intent['name']))\n",
    "        if intent['name'] not in classes:\n",
    "            classes.append(intent['name'])\n",
    "\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "pickle.dump(words, open('words_rnn.pkl', 'wb'))\n",
    "pickle.dump(classes, open('classes_rnn.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf03435",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "313/313 [==============================] - 72s 224ms/step - loss: 5.6149 - accuracy: 0.0019\n",
      "Epoch 2/200\n",
      "313/313 [==============================] - 72s 229ms/step - loss: 5.5933 - accuracy: 0.0090\n",
      "Epoch 3/200\n",
      "313/313 [==============================] - 89s 284ms/step - loss: 5.5789 - accuracy: 0.0083\n",
      "Epoch 4/200\n",
      "313/313 [==============================] - 97s 310ms/step - loss: 5.5705 - accuracy: 0.0096\n",
      "Epoch 5/200\n",
      "313/313 [==============================] - 94s 299ms/step - loss: 5.5631 - accuracy: 0.0115\n",
      "Epoch 6/200\n",
      "313/313 [==============================] - 76s 243ms/step - loss: 5.5405 - accuracy: 0.0135\n",
      "Epoch 7/200\n",
      "313/313 [==============================] - 78s 249ms/step - loss: 5.5230 - accuracy: 0.0147\n",
      "Epoch 8/200\n",
      "313/313 [==============================] - 78s 249ms/step - loss: 5.5303 - accuracy: 0.0122\n",
      "Epoch 9/200\n",
      "313/313 [==============================] - 76s 244ms/step - loss: 5.5212 - accuracy: 0.0186\n",
      "Epoch 10/200\n",
      "313/313 [==============================] - 78s 250ms/step - loss: 5.5242 - accuracy: 0.0141\n",
      "Epoch 11/200\n",
      "313/313 [==============================] - 79s 253ms/step - loss: 5.5115 - accuracy: 0.0179\n",
      "Epoch 12/200\n",
      "313/313 [==============================] - 78s 249ms/step - loss: 5.4929 - accuracy: 0.0154\n",
      "Epoch 13/200\n",
      "313/313 [==============================] - 78s 250ms/step - loss: 5.4837 - accuracy: 0.0173\n",
      "Epoch 14/200\n",
      "313/313 [==============================] - 108s 344ms/step - loss: 5.4686 - accuracy: 0.0211\n",
      "Epoch 15/200\n",
      "313/313 [==============================] - 107s 342ms/step - loss: 5.4454 - accuracy: 0.0179\n",
      "Epoch 16/200\n",
      "313/313 [==============================] - 94s 301ms/step - loss: 5.4291 - accuracy: 0.0179\n",
      "Epoch 17/200\n",
      "313/313 [==============================] - 96s 306ms/step - loss: 5.4239 - accuracy: 0.0192\n",
      "Epoch 18/200\n",
      "313/313 [==============================] - 91s 290ms/step - loss: 5.4147 - accuracy: 0.0218\n",
      "Epoch 19/200\n",
      "313/313 [==============================] - 95s 303ms/step - loss: 5.4232 - accuracy: 0.0218\n",
      "Epoch 20/200\n",
      "313/313 [==============================] - 96s 306ms/step - loss: 5.4245 - accuracy: 0.0192\n",
      "Epoch 21/200\n",
      "313/313 [==============================] - 95s 304ms/step - loss: 5.3872 - accuracy: 0.0186\n",
      "Epoch 22/200\n",
      "313/313 [==============================] - 82s 262ms/step - loss: 5.3736 - accuracy: 0.0231\n",
      "Epoch 23/200\n",
      "313/313 [==============================] - 78s 251ms/step - loss: 5.3335 - accuracy: 0.0218\n",
      "Epoch 24/200\n",
      "313/313 [==============================] - 87s 279ms/step - loss: 5.3278 - accuracy: 0.0224\n",
      "Epoch 25/200\n",
      "313/313 [==============================] - 89s 285ms/step - loss: 5.3077 - accuracy: 0.0263\n",
      "Epoch 26/200\n",
      "313/313 [==============================] - 82s 262ms/step - loss: 5.3043 - accuracy: 0.0256\n",
      "Epoch 27/200\n",
      "313/313 [==============================] - 80s 254ms/step - loss: 5.2743 - accuracy: 0.0250\n",
      "Epoch 28/200\n",
      "313/313 [==============================] - 79s 251ms/step - loss: 5.2589 - accuracy: 0.0314\n",
      "Epoch 29/200\n",
      "313/313 [==============================] - 77s 245ms/step - loss: 5.2374 - accuracy: 0.0295\n",
      "Epoch 30/200\n",
      "313/313 [==============================] - 79s 254ms/step - loss: 5.2200 - accuracy: 0.0301\n",
      "Epoch 31/200\n",
      "313/313 [==============================] - 90s 287ms/step - loss: 5.1979 - accuracy: 0.0301\n",
      "Epoch 32/200\n",
      "313/313 [==============================] - 85s 270ms/step - loss: 5.1633 - accuracy: 0.0333\n",
      "Epoch 33/200\n",
      "313/313 [==============================] - 1530s 5s/step - loss: 5.1676 - accuracy: 0.0307\n",
      "Epoch 34/200\n",
      "313/313 [==============================] - 75s 241ms/step - loss: 5.1319 - accuracy: 0.0333\n",
      "Epoch 35/200\n",
      "313/313 [==============================] - 72s 229ms/step - loss: 5.5943 - accuracy: 0.0231\n",
      "Epoch 36/200\n",
      "313/313 [==============================] - 71s 226ms/step - loss: 6.0321 - accuracy: 0.0141\n",
      "Epoch 37/200\n",
      "313/313 [==============================] - 77s 248ms/step - loss: 5.8504 - accuracy: 0.0135\n",
      "Epoch 38/200\n",
      "313/313 [==============================] - 81s 257ms/step - loss: 5.8989 - accuracy: 0.0160\n",
      "Epoch 39/200\n",
      "313/313 [==============================] - 78s 248ms/step - loss: 5.8319 - accuracy: 0.0167\n",
      "Epoch 40/200\n",
      "313/313 [==============================] - 77s 245ms/step - loss: 5.7250 - accuracy: 0.0160\n",
      "Epoch 41/200\n",
      "313/313 [==============================] - 83s 264ms/step - loss: 5.7104 - accuracy: 0.0147\n",
      "Epoch 42/200\n",
      "313/313 [==============================] - 83s 266ms/step - loss: 5.7080 - accuracy: 0.0160\n",
      "Epoch 43/200\n",
      "313/313 [==============================] - 83s 266ms/step - loss: 5.6441 - accuracy: 0.0167\n",
      "Epoch 44/200\n",
      "313/313 [==============================] - 82s 263ms/step - loss: 5.6560 - accuracy: 0.0154\n",
      "Epoch 45/200\n",
      "313/313 [==============================] - 84s 269ms/step - loss: 5.6528 - accuracy: 0.0154\n",
      "Epoch 46/200\n",
      "313/313 [==============================] - 80s 257ms/step - loss: 5.5767 - accuracy: 0.0173\n",
      "Epoch 47/200\n",
      "313/313 [==============================] - 83s 266ms/step - loss: 5.5876 - accuracy: 0.0179\n",
      "Epoch 48/200\n",
      "313/313 [==============================] - 79s 251ms/step - loss: 5.5453 - accuracy: 0.0211\n",
      "Epoch 49/200\n",
      "313/313 [==============================] - 81s 257ms/step - loss: 5.5528 - accuracy: 0.0154\n",
      "Epoch 50/200\n",
      "313/313 [==============================] - 78s 248ms/step - loss: 5.5330 - accuracy: 0.0205\n",
      "Epoch 51/200\n",
      "313/313 [==============================] - 77s 245ms/step - loss: 5.5005 - accuracy: 0.0205\n",
      "Epoch 52/200\n",
      "313/313 [==============================] - 83s 266ms/step - loss: 5.4774 - accuracy: 0.0173\n",
      "Epoch 53/200\n",
      "313/313 [==============================] - 79s 252ms/step - loss: 5.4885 - accuracy: 0.0205\n",
      "Epoch 54/200\n",
      "313/313 [==============================] - 78s 249ms/step - loss: 5.4900 - accuracy: 0.0192\n",
      "Epoch 55/200\n",
      "313/313 [==============================] - 77s 245ms/step - loss: 5.4853 - accuracy: 0.0179\n",
      "Epoch 56/200\n",
      "313/313 [==============================] - 77s 245ms/step - loss: 5.4661 - accuracy: 0.0173\n",
      "Epoch 57/200\n",
      "313/313 [==============================] - 76s 244ms/step - loss: 5.4498 - accuracy: 0.0186\n",
      "Epoch 58/200\n",
      "313/313 [==============================] - 81s 260ms/step - loss: 5.4277 - accuracy: 0.0218\n",
      "Epoch 59/200\n",
      "313/313 [==============================] - 77s 247ms/step - loss: 5.4116 - accuracy: 0.0211\n",
      "Epoch 60/200\n",
      "313/313 [==============================] - 77s 245ms/step - loss: 5.3932 - accuracy: 0.0250\n",
      "Epoch 61/200\n",
      "313/313 [==============================] - 77s 246ms/step - loss: 5.4699 - accuracy: 0.0243\n",
      "Epoch 62/200\n",
      "313/313 [==============================] - 78s 249ms/step - loss: 6.1713 - accuracy: 0.0115\n",
      "Epoch 63/200\n",
      "313/313 [==============================] - 77s 245ms/step - loss: 5.8757 - accuracy: 0.0135\n",
      "Epoch 64/200\n",
      "313/313 [==============================] - 80s 255ms/step - loss: 5.8425 - accuracy: 0.0109\n",
      "Epoch 65/200\n",
      "313/313 [==============================] - 81s 260ms/step - loss: 5.8090 - accuracy: 0.0115\n",
      "Epoch 66/200\n",
      "313/313 [==============================] - 80s 256ms/step - loss: 5.7848 - accuracy: 0.0115\n",
      "Epoch 67/200\n",
      "313/313 [==============================] - 76s 244ms/step - loss: 5.7742 - accuracy: 0.0109\n",
      "Epoch 68/200\n",
      "313/313 [==============================] - 90s 288ms/step - loss: 5.7426 - accuracy: 0.0147\n",
      "Epoch 69/200\n",
      "313/313 [==============================] - 78s 248ms/step - loss: 5.7316 - accuracy: 0.0186\n",
      "Epoch 70/200\n",
      "313/313 [==============================] - 77s 247ms/step - loss: 5.7028 - accuracy: 0.0160\n",
      "Epoch 71/200\n",
      "120/313 [==========>...................] - ETA: 49s - loss: 5.7010 - accuracy: 0.0133"
     ]
    }
   ],
   "source": [
    "training_rnn = []\n",
    "output_empty_rnn = [0] * len(classes)\n",
    "\n",
    "for doc in documents:\n",
    "    bag_rnn = []\n",
    "    pattern_words_rnn = doc[0]\n",
    "    pattern_words_rnn = [lemmatizer.lemmatize(word.lower()) for word in pattern_words_rnn]\n",
    "    for w in words:\n",
    "        bag_rnn.append(1) if w in pattern_words_rnn else bag_rnn.append(0)\n",
    "\n",
    "    output_row_rnn = list(output_empty_rnn)\n",
    "    output_row_rnn[classes.index(doc[1])] = 1\n",
    "\n",
    "    training_rnn.append([np.array(bag_rnn), np.array(output_row_rnn)])\n",
    "\n",
    "train_x_rnn = [item[0] for item in training_rnn]\n",
    "train_y_rnn = [item[1] for item in training_rnn]\n",
    "\n",
    "train_x_rnn = pad_sequences(train_x_rnn)  # Apply pad_sequences here\n",
    "train_y_rnn = np.array(train_y_rnn)\n",
    "\n",
    "model_rnn = Sequential()\n",
    "model_rnn.add(Embedding(input_dim=len(words), output_dim=128, input_length=train_x_rnn.shape[1]))\n",
    "model_rnn.add(LSTM(128))\n",
    "model_rnn.add(Dropout(0.5))\n",
    "model_rnn.add(Dense(len(classes), activation='softmax'))\n",
    "\n",
    "model_rnn.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "hist_rnn = model_rnn.fit(train_x_rnn, train_y_rnn, epochs=200, batch_size=5, verbose=1)\n",
    "final_training_accuracy_rnn = hist_rnn.history['accuracy'][-1]\n",
    "print(f'RNN Model - Final Training Accuracy: {final_training_accuracy_rnn * 100:.2f}%')\n",
    "\n",
    "plt.plot(hist_rnn.history['accuracy'])\n",
    "plt.title('RNN Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "model_rnn.save('chatbot_model_rnn.h5', hist_rnn)\n",
    "print(\"RNN Model created\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
